{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRjqmnn3fvGV"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rHhDT5BVulDd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import pickle\n",
        "import gzip\n",
        "import copy\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oMVLuk-Fe7PG"
      },
      "outputs": [],
      "source": [
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZJtjBLgpmrR"
      },
      "source": [
        "#Misc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XZiS_GXhpnvc"
      },
      "outputs": [],
      "source": [
        "def KL_DIV(mu0,rho0,mu1,rho1):\n",
        "  std1 = torch.exp(0.5*rho1)\n",
        "  std0 = torch.exp(0.5*rho0)\n",
        "  kl_div = (0.5*((std0/std1)**2+((mu0-mu1)**2/(std1**2))-1+2*torch.log(std1/std0))).sum()\n",
        "  return kl_div\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDOykI-zflfB"
      },
      "source": [
        "#Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEe-B0YAf-YP"
      },
      "source": [
        "##Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "4DWJarQVfmfc"
      },
      "outputs": [],
      "source": [
        "class BayesianLinear(nn.Module):\n",
        "    def __init__(self, input_size, output_size,device,init_rho,prior_rho,dist=\"gaussian\"):\n",
        "        super(BayesianLinear, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.device=device\n",
        "        self.dist=dist\n",
        "\n",
        "        self.post_w_mu = nn.Parameter(torch.zeros(output_size, input_size))\n",
        "        self.post_b_mu = nn.Parameter(torch.zeros(output_size))\n",
        "\n",
        "        self.post_w_rho = nn.Parameter(torch.full((output_size, input_size), init_rho))\n",
        "        self.post_b_rho = nn.Parameter(torch.full((output_size,), init_rho))\n",
        "\n",
        "        self.prior_w_mu = torch.zeros(output_size, input_size).to(device)\n",
        "        self.prior_b_mu = torch.zeros(output_size).to(device)\n",
        "\n",
        "        self.prior_w_rho = torch.full((output_size, input_size), prior_rho).to(device)\n",
        "        self.prior_b_rho = torch.full((output_size,), prior_rho).to(device)\n",
        "        rate = torch.tensor(1.0, device=device)\n",
        "        self.exp_dist=torch.distributions.Exponential(rate=rate)\n",
        "        self.init_parameters()\n",
        "\n",
        "\n",
        "    def to(self, device):\n",
        "        super(BayesianLinear, self).to(device)\n",
        "        self.prior_w_mu = self.prior_w_mu.to(device)\n",
        "        self.prior_b_mu = self.prior_b_mu.to(device)\n",
        "        self.prior_w_rho = self.prior_w_rho.to(device)\n",
        "        self.prior_b_rho = self.prior_b_rho.to(device)\n",
        "\n",
        "\n",
        "    def init_parameters(self):\n",
        "        nn.init.trunc_normal_(self.post_w_mu,std=0.1)\n",
        "        nn.init.trunc_normal_(self.post_b_mu,std=0.1)\n",
        "        nn.init.trunc_normal_(self.prior_w_mu,std=0.1)\n",
        "        nn.init.trunc_normal_(self.prior_b_mu,std=0.1)\n",
        "\n",
        "\n",
        "    def forward(self,x,T):\n",
        "      a = self.sample_activation(x) #(T,batch_size,output_size)\n",
        "      b = self.sample_b(x=x) #(T,1,output_size)\n",
        "      return a+b #(T,batch_size,output_size)\n",
        "\n",
        "\n",
        "    def sample_activation(self,x):\n",
        "      if self.dist==\"gaussian\":\n",
        "        act_mean = F.linear(x,self.post_w_mu) #(T,batch_size,output_size)\n",
        "        std = torch.exp(0.5*self.post_w_rho) #(output_size,input_size)\n",
        "        act_std = torch.sqrt(1e-8+F.linear((x**2),(std**2))) #(T,batch_size,output_size)\n",
        "        act_eps = torch.randn(x.shape[0],1,self.output_size).to(self.device) #(T,batch_size,output_size)\n",
        "        act = act_mean+act_std*act_eps #(T,batch_size,output_size)\n",
        "        return act #(T,batch_size,output_size)\n",
        "      if self.dist==\"radial\":\n",
        "        eps=torch.randn((x.shape[0],*self.post_w_rho.shape),device=self.device)\n",
        "        eps=eps/torch.linalg.norm(eps,dim=(1,2)).view(-1,1,1)\n",
        "        weights=self.post_w_mu.unsqueeze(0)+torch.randn((x.shape[0],1,1),device=self.device)*eps*torch.exp(0.5*self.post_w_rho.unsqueeze(0)) #(T,output_size,input_size)\n",
        "        act=weights.unsqueeze(1)@x.unsqueeze(-1) #(T,1,output_size,input_size)@(T,batch_size,input_size,1)=#(T,1,output_size,1)\n",
        "        return act.squeeze(-1)\n",
        "      if self.dist==\"laplace\":\n",
        "        bernoulli=2*torch.bernoulli(torch.full((x.shape[0],*self.post_w_rho.shape),0.5,device=self.device))-1\n",
        "        eps=self.exp_dist.sample((x.shape[0],*self.post_w_rho.shape))\n",
        "        weights=(self.post_w_mu.unsqueeze(0)+bernoulli*eps*torch.exp(0.5*self.post_w_rho.unsqueeze(0))).unsqueeze(1) #(T,1,output_size,input_size)\n",
        "        act=weights@x.unsqueeze(-1) #(T,batch_size,input_size,1)\n",
        "        return act.squeeze(-1) #(T,batch_size,input_size)\n",
        "\n",
        "\n",
        "    def sample_weights_radial(self,n_samples):\n",
        "      eps=torch.randn((n_samples,*self.post_w_rho.shape),device=self.device)\n",
        "      eps=eps/torch.linalg.norm(eps,dim=(1,2)).view(-1,1,1)\n",
        "      r=torch.randn((eps.shape[0],1,1),device=self.device)\n",
        "      weights=self.post_w_mu.view(1,*self.post_w_mu.shape)+r*eps*torch.exp(0.5*self.post_w_rho).unsqueeze(0)\n",
        "      return weights\n",
        "\n",
        "\n",
        "    def Lcrossentropy(self,n_samples):\n",
        "      norms=torch.linalg.norm((self.sample_weights_radial(n_samples)-self.prior_w_mu.unsqueeze(0))/torch.exp(0.5*self.prior_w_rho.unsqueeze(0)),dim=(1,2))\n",
        "      L=-0.5*torch.mean(norms**2)\n",
        "      L+=-(self.prior_w_mu.numel()-1)*torch.mean(torch.log(norms))\n",
        "\n",
        "      x=torch.ones(n_samples,1,1)\n",
        "      normsb=torch.linalg.norm((self.sample_b(x).squeeze(1)-self.prior_b_mu.unsqueeze(0))/torch.exp(0.5*self.prior_b_rho.unsqueeze(0)),dim=1)\n",
        "      Lb=-0.5*torch.mean(normsb**2)\n",
        "      Lb+=-(self.prior_b_mu.numel()-1)*torch.mean(torch.log(normsb))\n",
        "\n",
        "      return L+Lb\n",
        "\n",
        "\n",
        "    def Lentropy(self,n_samples):\n",
        "      x=torch.ones(n_samples,1,1)\n",
        "      norms=torch.linalg.norm((self.sample_weights_radial(n_samples)-self.post_w_mu.unsqueeze(0))/torch.exp(0.5*self.post_w_rho.unsqueeze(0)),dim=(1,2))\n",
        "      L=-torch.log(torch.exp(0.5*self.post_w_rho)).sum()\n",
        "\n",
        "      sample=self.sample_b(x).squeeze(1)\n",
        "      normsb=torch.linalg.norm((sample-self.post_b_mu.unsqueeze(0))/torch.exp(0.5*self.post_b_rho.unsqueeze(0)),dim=1)\n",
        "      Lb=-torch.log(torch.exp(0.5*self.post_b_rho)).sum()\n",
        "      return L+Lb\n",
        "\n",
        "\n",
        "    def sample_b(self,x):\n",
        "      if self.dist==\"gaussian\":\n",
        "        b_eps = torch.randn(x.shape[0],self.output_size).to(self.device)\n",
        "        b = self.post_b_mu.unsqueeze(0)+torch.exp(0.5*self.post_b_rho.unsqueeze(0))*b_eps\n",
        "        return b.unsqueeze(1)\n",
        "      if self.dist==\"radial\":\n",
        "        b_eps = torch.randn(x.shape[0],self.output_size).to(self.device)\n",
        "        b_eps=b_eps/torch.linalg.norm(b_eps,dim=1).view(-1,1)\n",
        "        b = self.post_b_mu.unsqueeze(0)+torch.randn((x.shape[0],1),device=self.device)*torch.exp(0.5*self.post_b_rho.unsqueeze(0))*b_eps\n",
        "        return b.unsqueeze(1)\n",
        "      if self.dist==\"laplace\":\n",
        "        bernoulli=2*torch.bernoulli(torch.full((x.shape[0],1,*self.post_b_rho.shape),0.5,device=self.device))-1\n",
        "        eps=self.exp_dist.sample((x.shape[0],1,*self.post_b_rho.shape)).to(self.device)\n",
        "        b=self.post_b_mu.view(1,1,-1)+bernoulli*eps*torch.exp(0.5*self.post_b_rho.view(1,1,-1)) #(T,1,output_size)\n",
        "        return b #(T,1,output_size)\n",
        "\n",
        "    def update_prior(self):\n",
        "      self.prior_w_mu=copy.deepcopy(self.post_w_mu.data)\n",
        "      self.prior_w_rho=copy.deepcopy(self.post_w_rho.data)\n",
        "      self.prior_b_mu=copy.deepcopy(self.post_b_mu.data)\n",
        "      self.prior_b_rho=copy.deepcopy(self.post_b_rho.data)\n",
        "\n",
        "    def kl_div(self):\n",
        "      if self.dist==\"gaussian\":\n",
        "        return(KL_DIV(self.post_w_mu,self.post_w_rho,self.prior_w_mu,self.prior_w_rho)+KL_DIV(self.post_b_mu,self.post_b_rho,self.prior_b_mu,self.prior_b_rho))\n",
        "      if self.dist==\"laplace\":\n",
        "        kl_div=0.\n",
        "        bpost=torch.exp(0.5*self.post_w_rho)\n",
        "        bprior=torch.exp(0.5*self.prior_w_rho)\n",
        "        kl_div+=torch.log(bprior/bpost).sum()\n",
        "        kl_div+=((bpost*torch.exp(-torch.abs(self.post_w_mu-self.prior_w_mu)/bpost)+torch.abs(self.post_w_mu-self.prior_w_mu))/bprior).sum()\n",
        "        bpost=torch.exp(0.5*self.post_b_rho)\n",
        "        bprior=torch.exp(0.5*self.prior_b_rho)\n",
        "        kl_div+=torch.log(bprior/bpost).sum()\n",
        "        kl_div+=((bpost*torch.exp(-torch.abs(self.post_b_mu-self.prior_b_mu)/bpost)+torch.abs(self.post_b_mu-self.prior_b_mu))/bprior).sum()\n",
        "        return kl_div\n",
        "\n",
        "class NonBayesianLinear(nn.Module):\n",
        "    def __init__(self, input_size, output_size,device):\n",
        "        super(NonBayesianLinear, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.device=device\n",
        "\n",
        "        self.w = nn.Parameter(torch.empty(output_size, input_size))\n",
        "        self.b = nn.Parameter(torch.empty(output_size))\n",
        "\n",
        "        self.init_parameters()\n",
        "\n",
        "\n",
        "    def to(self, device):\n",
        "        super(BayesianLinear, self).to(device)\n",
        "        self.w = self.w.to(device)\n",
        "        self.b = self.b.to(device)\n",
        "\n",
        "\n",
        "    def init_parameters(self):\n",
        "        nn.init.trunc_normal_(self.w,std=0.1)\n",
        "        nn.init.trunc_normal_(self.b,std=0.1)\n",
        "\n",
        "    def forward(self,x,T):\n",
        "      return F.linear(x,self.w,self.b)\n",
        "    def update_prior(self):\n",
        "      pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfSYIEvROEly"
      },
      "source": [
        "##Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "JfSFztLOBfSJ"
      },
      "outputs": [],
      "source": [
        "class BayesianFC(nn.Module):\n",
        "  def __init__(self,input_size,output_size,hidden_size,n_hidden,device,init_rho,prior_rho,n_fisher_samples=3000,dist=\"gaussian\",output=\"categorical\",bayesian=True):\n",
        "    super(BayesianFC,self).__init__()\n",
        "    self.device=device\n",
        "    self.layers=nn.ModuleList()\n",
        "    if bayesian:\n",
        "      if dist==\"laplace\":\n",
        "        self.layers.append(BayesianLinear(input_size,hidden_size,device,init_rho,prior_rho,dist=dist))\n",
        "        for i in range(n_hidden-1):\n",
        "          self.layers.append(BayesianLinear(hidden_size,hidden_size,device,init_rho,prior_rho,dist=dist))\n",
        "        self.layers.append(BayesianLinear(hidden_size,output_size,device,init_rho,prior_rho,dist=dist))\n",
        "      else:\n",
        "        self.layers.append(BayesianLinear(input_size,hidden_size,device,init_rho,prior_rho,dist=dist))\n",
        "        for i in range(n_hidden-1):\n",
        "          self.layers.append(BayesianLinear(hidden_size,hidden_size,device,init_rho,prior_rho,dist=dist))\n",
        "        self.layers.append(BayesianLinear(hidden_size,output_size,device,init_rho,prior_rho,dist=dist))\n",
        "    else:\n",
        "      print(\"Non Bayesian network\")\n",
        "      self.layers.append(NonBayesianLinear(input_size,hidden_size,device))\n",
        "      for i in range(n_hidden-1):\n",
        "        self.layers.append(NonBayesianLinear(hidden_size,hidden_size,device))\n",
        "      self.layers.append(NonBayesianLinear(hidden_size,output_size,device))\n",
        "    self.bn = nn.BatchNorm1d(100)\n",
        "    self.n_fishersamples=n_fisher_samples\n",
        "    self.fisher_dicts=[]\n",
        "    self.param_history=[]\n",
        "    self.SI=None\n",
        "    self.dist=dist\n",
        "    self.output=output\n",
        "    self.grad={}\n",
        "\n",
        "\n",
        "  def transfer_weights(self,nonbayesian_model):\n",
        "    for i,layer in enumerate(self.layers):\n",
        "      layer.post_w_mu.data=nonbayesian_model.layers[i].w.data\n",
        "      layer.post_b_mu.data=nonbayesian_model.layers[i].b.data\n",
        "      layer.to(self.device)\n",
        "\n",
        "\n",
        "  def forward(self,x,T):\n",
        "    batch_size=x.shape[0]\n",
        "    if bayesian:\n",
        "      x=x.unsqueeze(0).tile((T,1,1))\n",
        "    else:\n",
        "      x=x.unsqueeze(0)\n",
        "    for i,layer in enumerate(self.layers):\n",
        "      x=layer(x,T)\n",
        "      if i<len(self.layers)-1:\n",
        "        x=F.relu(x,T)\n",
        "    output_dim = x.shape[-1]\n",
        "    if self.output==\"categorical\":\n",
        "      return F.log_softmax(x, dim=-1)\n",
        "    elif self.output==\"gaussian\":\n",
        "      return x[:,:,:10],torch.ones(x[:,:,10:].shape,device=self.device)*(-20)\n",
        "\n",
        "  def kl_div(self,n_data,n_samples=100):\n",
        "    if self.dist==\"gaussian\" or self.dist==\"laplace\":\n",
        "      total_kl = 0\n",
        "      for layer in self.layers:\n",
        "          total_kl += layer.kl_div()\n",
        "      return total_kl/n_data\n",
        "    if self.dist==\"radial\":\n",
        "      L_entropy=0\n",
        "      L_cross_entropy=0\n",
        "      for layer in self.layers:\n",
        "        L_entropy+=layer.Lentropy(n_samples)\n",
        "        L_cross_entropy+=layer.Lcrossentropy(n_samples)\n",
        "\n",
        "      return (L_entropy-L_cross_entropy)/n_data\n",
        "\n",
        "\n",
        "  def calculate_fim(self,train_loader1):\n",
        "    #Fisher information matrix calculation\n",
        "    fisher_dic = {}\n",
        "\n",
        "    for n, p in self.named_parameters():\n",
        "      fisher_dic[n] = p.detach().clone().zero_()\n",
        "\n",
        "    self.eval()\n",
        "    for batch_id, (x, y) in enumerate(train_loader1):\n",
        "      if batch_id >= self.n_fishersamples:\n",
        "        break\n",
        "      x,y=x.to(self.device),y.to(self.device)\n",
        "      loss = self.nll(x,y)\n",
        "      self.zero_grad()\n",
        "      loss.backward()\n",
        "      for n, p in self.named_parameters():\n",
        "        if p.requires_grad and p.grad is not None:\n",
        "          fisher_dic[n] += p.grad.detach() ** 2\n",
        "\n",
        "    fisher_dic = {n: p/batch_id for n,p in fisher_dic.items()}\n",
        "    self.fisher_dicts.append(fisher_dic)\n",
        "    param_dic = {}\n",
        "    for n, p in self.named_parameters():\n",
        "      param_dic[n] = p.detach().clone()\n",
        "    self.param_history.append(param_dic)\n",
        "    self.train()\n",
        "\n",
        "\n",
        "  def record_grads(self):\n",
        "    for n, p in self.named_parameters():\n",
        "      if p.requires_grad and p.grad is not None:\n",
        "        if n in self.grad:\n",
        "          self.grad[n].append(p.grad.detach())\n",
        "        else:\n",
        "          self.grad[n]=[p.grad.detach()]\n",
        "\n",
        "\n",
        "  def std(self):\n",
        "    var={}\n",
        "    sum_std=0\n",
        "    for n, p in self.named_parameters():\n",
        "      if p.requires_grad and p.grad is not None:\n",
        "        grads=torch.stack(self.grad[n])\n",
        "        grads=grads.view(grads.shape[0],-1)\n",
        "        print(grads.shape)\n",
        "        stds=grads.std(dim=0)\n",
        "    sum_std+=stds.sum()\n",
        "    self.grad={}\n",
        "    return sum_std\n",
        "\n",
        "\n",
        "  def calculate_ewc(self):\n",
        "    #Calculation of EWC loss\n",
        "    Lloss = []\n",
        "    for task in range(len(self.fisher_dicts)):\n",
        "      for n, p in self.named_parameters():\n",
        "        if p.requires_grad:\n",
        "          Lloss.append( torch.sum( self.fisher_dicts[task][n] * ( p-self.param_history[task][n] ) ** 2 ))\n",
        "\n",
        "    return (1./2)*sum(Lloss)\n",
        "\n",
        "\n",
        "  def update_prior(self):\n",
        "    for layer in self.layers:\n",
        "        layer.update_prior()\n",
        "\n",
        "\n",
        "  def elbo(self,x,y,n_data,T=10):\n",
        "    nll=self.nll(x,y,T=T)\n",
        "    kl=self.kl_div(n_data)\n",
        "    return nll+kl\n",
        "\n",
        "\n",
        "  def nll(self,x,y,T=10):\n",
        "    if self.output==\"categorical\":\n",
        "      log_probs=self.forward(x,T=T).mean(dim=0)\n",
        "      nll=F.nll_loss(log_probs,y,reduction=\"mean\")\n",
        "      return nll\n",
        "    if self.output==\"gaussian\":\n",
        "      mu,rho=self.forward(x,T=T)\n",
        "      sigma=torch.exp(0.5*rho)\n",
        "      y_one_hot=F.one_hot(y,num_classes=10).unsqueeze(0)\n",
        "      log_term=torch.sum(-torch.log(sigma),dim=-1)\n",
        "      square_term=torch.sum(-(1/(2*(sigma**2)))*(y_one_hot-mu)**2,dim=-1)\n",
        "      nll=(-log_term-square_term).mean()\n",
        "      return nll\n",
        "\n",
        "\n",
        "  def rmse_gaussian(self,x,y,T=10):\n",
        "    if self.output==\"gaussian\":\n",
        "      mu,rho=self.forward(x,T)\n",
        "      sigma=torch.exp(0.5*rho)\n",
        "      y_one_hot=F.one_hot(y,num_classes=10).unsqueeze(0)\n",
        "      rmse=torch.sqrt(torch.mean(torch.sum(sigma**2+(mu-y_one_hot)**2,dim=-1)))\n",
        "      return rmse\n",
        "    else:\n",
        "      raise ValueError(\"function and output type not matching\")\n",
        "\n",
        "\n",
        "class MultiBayesianFC(nn.Module):\n",
        "\n",
        "  def __init__(self,input_size,hidden_size,n_hidden,device,n_tasks=5,n_fisher_samples=3000):\n",
        "    super(MultiBayesianFC,self).__init__()\n",
        "    self.device=device\n",
        "    self.layers=nn.ModuleList()\n",
        "    self.layers.append(BayesianLinear(input_size,hidden_size,device))\n",
        "    for i in range(n_hidden-1):\n",
        "      self.layers.append(BayesianLinear(hidden_size,hidden_size,device))\n",
        "    self.last_layers=nn.ModuleList()\n",
        "    for i in range (n_tasks):\n",
        "      self.last_layers.append(BayesianLinear(hidden_size,2,device))\n",
        "    self.bn = nn.BatchNorm1d(100)\n",
        "    self.n_fishersamples=n_fisher_samples\n",
        "    self.fisher_dicts=[]\n",
        "    self.param_history=[]\n",
        "    self.SI=None\n",
        "\n",
        "\n",
        "  def transfer_weights(self,nonbayesian_model):\n",
        "    for i,layer in enumerate(self.layers):\n",
        "      layer.post_w_mu.data=nonbayesian_model.layers[i].w.data\n",
        "      layer.post_b_mu.data=nonbayesian_model.layers[i].b.data\n",
        "      layer.to(self.device)\n",
        "    for i,layer in enumerate(self.last_layers):\n",
        "      layer.post_w_mu.data=nonbayesian_model.layers[i].w.data\n",
        "      layer.post_b_mu.data=nonbayesian_model.layers[i].b.data\n",
        "      layer.to(self.device)\n",
        "\n",
        "\n",
        "  def forward(self,x,id_task,n_samples=1):\n",
        "    x_tiled=x.tile((n_samples, 1))\n",
        "    for i,layer in enumerate(self.layers):\n",
        "      x_tiled=F.relu(layer(x_tiled))\n",
        "    x_tiled=self.last_layers[id_task](x_tiled)\n",
        "    output_dim = x_tiled.shape[-1]\n",
        "    x_tiled = x_tiled.view(n_samples, -1, output_dim)\n",
        "\n",
        "    return F.log_softmax(x_tiled, dim=-1).mean(dim=0)\n",
        "\n",
        "\n",
        "  def kl_div(self,n_data):\n",
        "    total_kl = 0\n",
        "    for layer in self.layers:\n",
        "        total_kl += layer.kl_div()\n",
        "    return total_kl/n_data\n",
        "\n",
        "\n",
        "  def calculate_fim(self,train_loader1,task_id):\n",
        "    #Fisher information matrix calculation\n",
        "    fisher_dic = {}\n",
        "\n",
        "    for n, p in self.named_parameters():\n",
        "      if n.startswith(\"layers\"):\n",
        "        fisher_dic[n] = p.detach().clone().zero_()\n",
        "\n",
        "    self.eval()\n",
        "    for batch_id, (x, y) in enumerate(train_loader1):\n",
        "      if batch_id >= self.n_fishersamples:\n",
        "        break\n",
        "      x,y=x.to(self.device),y.to(self.device)\n",
        "      loss = self.nll(x,y,task_id)\n",
        "      self.zero_grad()\n",
        "      loss.backward()\n",
        "      for n, p in self.named_parameters():\n",
        "        if n.startswith(\"layers\"):\n",
        "          if p.requires_grad and p.grad is not None:\n",
        "            fisher_dic[n] += p.grad.detach() ** 2\n",
        "\n",
        "    fisher_dic = {n: p/batch_id for n,p in fisher_dic.items()}\n",
        "    self.fisher_dicts.append(fisher_dic)\n",
        "    param_dic = {}\n",
        "    for n, p in self.named_parameters():\n",
        "      if n.startswith(\"layers\"):\n",
        "        param_dic[n] = p.detach().clone()\n",
        "    self.param_history.append(param_dic)\n",
        "    self.train()\n",
        "\n",
        "\n",
        "  def calculate_ewc(self):\n",
        "    #EWC loss calculation\n",
        "    Lloss = []\n",
        "    for task in range(len(self.fisher_dicts)):\n",
        "      for n, p in self.named_parameters():\n",
        "        if n.startswith(\"layers\"):\n",
        "          if p.requires_grad:\n",
        "            Lloss.append( torch.sum( self.fisher_dicts[task][n] * ( p-self.param_history[task][n] ) ** 2 ))\n",
        "\n",
        "    return (1./2)*sum(Lloss)\n",
        "\n",
        "\n",
        "  def update_prior(self):\n",
        "    for layer in self.layers:\n",
        "        layer.update_prior()\n",
        "    for layer in self.last_layers:\n",
        "        layer.update_prior()\n",
        "\n",
        "\n",
        "  def elbo(self,x,y,n_data,task_id,T=10):\n",
        "    x_tiled = x.tile((T, 1))  # (T * batch_size, input_size)\n",
        "    y_tiled = y.tile((T,))\n",
        "    nll=self.nll(x_tiled,y_tiled,task_id)\n",
        "    kl=self.kl_div(n_data)\n",
        "    return nll+kl\n",
        "\n",
        "\n",
        "  def nll(self,x,y,task_id):\n",
        "    log_probs=self.forward(x,task_id)\n",
        "    nll=F.nll_loss(log_probs,y,reduction=\"mean\")\n",
        "    return nll"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X46zSkUjBWN4"
      },
      "source": [
        "##SI-specific class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Nxw1Isll7gaI"
      },
      "outputs": [],
      "source": [
        "class SynapticIntelligence():\n",
        "  def __init__(self,model,si_c=0.1,epsilon=0.1,gamma=0.9):\n",
        "    self.prev_SI={}\n",
        "    self.omega_SI={}\n",
        "    self.W={}\n",
        "    self.p_old={}\n",
        "    self.model=model\n",
        "    self.si_c = si_c\n",
        "    self.epsilon = epsilon\n",
        "    self.gamma=gamma\n",
        "\n",
        "  def init_SI(self):\n",
        "    #initializing the dictionary\n",
        "    for n, p in self.model.named_parameters():\n",
        "      if n.startswith(\"layers\"):\n",
        "        if p.requires_grad:\n",
        "            self.prev_SI[n] = p.data.clone()\n",
        "\n",
        "  def prepare_w_P_SI(self):\n",
        "    #other initialization\n",
        "    for n, p in self.model.named_parameters():\n",
        "      if n.startswith(\"layers\"):\n",
        "        if p.requires_grad:\n",
        "            self.W[n] = p.data.clone().zero_()\n",
        "            self.p_old[n] = p.data.clone()\n",
        "\n",
        "  def update_parameter_importance(self):\n",
        "    #to compute matrixes which measure parameter importances\n",
        "    for n, p in self.model.named_parameters():\n",
        "      if n.startswith(\"layers\"):\n",
        "        if p.requires_grad:\n",
        "            if p.grad is not None:\n",
        "                self.W[n].add_(-p.grad*(p.detach()-self.p_old[n]))\n",
        "            self.p_old[n] = p.detach().clone()\n",
        "\n",
        "  def update_omega(self):\n",
        "    #using previous calculations to update omega\n",
        "        for n, p in self.model.named_parameters():\n",
        "          if n.startswith(\"layers\"):\n",
        "            if p.requires_grad:\n",
        "                p_prev = self.prev_SI[n]\n",
        "                p_current = p.detach().clone()\n",
        "                p_change = p_current - p_prev\n",
        "                omega_add = self.W[n]/(p_change**2 + self.epsilon)\n",
        "                try:\n",
        "                    omega = self.omega_SI[n]\n",
        "                except KeyError:\n",
        "                    omega = p.detach().clone().zero_()\n",
        "                omega_new = omega + omega_add\n",
        "\n",
        "                self.prev_SI[n] = p_current\n",
        "                self.omega_SI[n] = omega_new\n",
        "\n",
        "  def divide_omega(self):\n",
        "    pass\n",
        "    '''\n",
        "    for n, p in self.model.named_parameters():\n",
        "      if n.startswith(\"layers\"):\n",
        "        if p.requires_grad:\n",
        "          self.omega_SI[n]=self.omega_SI[n]*self.gamma'''\n",
        "\n",
        "  def surrogate_l(self):\n",
        "    #Calculation of SI loss\n",
        "        try:\n",
        "            losses = []\n",
        "            for n, p in self.model.named_parameters():\n",
        "              if n.startswith(\"layers\"):\n",
        "                if p.requires_grad:\n",
        "                    prev_values = self.prev_SI[n]\n",
        "                    omega = self.omega_SI[n]\n",
        "                    losses.append((omega * (p-prev_values)**2).sum())\n",
        "            return sum(losses)\n",
        "        except KeyError:\n",
        "            return torch.tensor(0., device=self._device())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luofCWfOXUDw"
      },
      "source": [
        "#Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yukzstOdgiM8"
      },
      "outputs": [],
      "source": [
        "def permute_MNIST(x_train,y_train,x_test,y_test,task_id):\n",
        "    np.random.seed(task_id)\n",
        "    permutation = np.random.permutation(x_train.shape[1])\n",
        "    return x_train[:, permutation], y_train, x_test[:, permutation], y_test\n",
        "\n",
        "def split_MNIST(x_train,y_train,x_test,y_test,task_id):\n",
        "    Tasks=[[0,1],[2,3],[4,5],[6,7],[8,9]]\n",
        "    classes=Tasks[task_id]\n",
        "    idxs_train=np.where(np.logical_or(y_train==classes[0],y_train==classes[1]))[0]\n",
        "    idxs_test=np.where(np.logical_or(y_test==classes[0],y_test==classes[1]))[0]\n",
        "    if task_id==1:\n",
        "      print(y_test[idxs_test])\n",
        "    return x_train[idxs_train],1*(y_train[idxs_train]==y_train[idxs_train][0]),x_test[idxs_test],1*(y_test[idxs_test]==y_train[idxs_train][0])\n",
        "\n",
        "\n",
        "''' Coreset methods '''\n",
        "\n",
        "def random_coreset(x_train, y_train, x_coreset, y_coreset, nb_coreset,dataset=\"split\"):\n",
        "\n",
        "    ids = np.random.choice(x_train.shape[0], nb_coreset, False)\n",
        "\n",
        "    x_coreset.append(x_train[ids,:])\n",
        "    y_coreset.append(y_train[ids])\n",
        "\n",
        "    x_train = np.delete(x_train, ids, axis=0)\n",
        "    y_train = np.delete(y_train, ids, axis=0)\n",
        "\n",
        "    return x_train, y_train, x_coreset, y_coreset\n",
        "\n",
        "def k_center(x_train, y_train, x_coreset, y_coreset, nb_coreset,dataset=\"split\"):\n",
        "\n",
        "    ids = []\n",
        "    visited = np.ones(x_train.shape[0])\n",
        "    id = 0\n",
        "\n",
        "    while len(ids)<nb_coreset:\n",
        "        ds = dists(x_train,id)*visited\n",
        "        id = np.atleast_1d(np.argmax(ds))[0]\n",
        "        visited[id]=0\n",
        "        ids.append(id)\n",
        "\n",
        "    x_coreset.append(x_train[ids,:])\n",
        "    y_coreset.append(y_train[ids])\n",
        "\n",
        "    x_train = np.delete(x_train, ids, axis=0)\n",
        "    y_train = np.delete(y_train, ids, axis=0)\n",
        "\n",
        "    if dataset==\"permuted\":\n",
        "      return ids\n",
        "    else:\n",
        "      return x_train, y_train, x_coreset, y_coreset\n",
        "\n",
        "def dists(x_train, id):\n",
        "    return np.linalg.norm(x_train-x_train[id,:],axis=1)\n",
        "\n",
        "def get_total_elements(loader):\n",
        "  total_elements = 0\n",
        "  for batch_idx, (data, target) in enumerate(loader):\n",
        "      total_elements += data.shape[0]\n",
        "  return total_elements\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-U6YwBGXXWs"
      },
      "source": [
        "#Training functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2Ljcx5kwc95"
      },
      "source": [
        "##Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yHadFnDCXZ0A"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "def dataloader(X, Y, batch_size=128, shuffle=True, num_workers=0, pin_memory=False):\n",
        "    loaders = []\n",
        "    for x, y in zip(X, Y):\n",
        "        assert isinstance(x, np.ndarray) and isinstance(y, np.ndarray), \"X and Y must be NumPy arrays\"\n",
        "        dataset = TensorDataset(torch.from_numpy(x).float(), torch.from_numpy(y).long())\n",
        "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,\n",
        "                            num_workers=num_workers, pin_memory=pin_memory)\n",
        "        loaders.append(loader)\n",
        "    return loaders\n",
        "\n",
        "def aggdataloader(X, Y, batch_size=128, shuffle=True, num_workers=0, pin_memory=False):\n",
        "    loaders = []\n",
        "    x_agg,y_agg=None,None\n",
        "    for i,(x, y) in enumerate(zip(X, Y)):\n",
        "        if i==0:\n",
        "          x_agg=x\n",
        "          y_agg=y\n",
        "        else:\n",
        "          x_agg=np.concatenate((x_agg,x),axis=0)\n",
        "          y_agg=np.concatenate((y_agg,y),axis=0)\n",
        "        assert isinstance(x, np.ndarray) and isinstance(y, np.ndarray), \"X and Y must be NumPy arrays\"\n",
        "        dataset = TensorDataset(torch.from_numpy(x_agg).float(), torch.from_numpy(y_agg).long())\n",
        "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,\n",
        "                            num_workers=num_workers, pin_memory=pin_memory)\n",
        "        loaders.append(loader)\n",
        "    return loaders\n",
        "\n",
        "\n",
        "\n",
        "def loaders(nb_tasks,coreset_size,batch_size=128,dataset=\"permuted\"):\n",
        "  with gzip.open('sample_data/mnist.pkl.gz', 'rb') as f:\n",
        "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
        "  X_trains,Y_trains,Xk_trains,Yk_trains,X_tests,Y_tests=[],[],[],[],[],[]\n",
        "  for i in range(nb_tasks):\n",
        "    if dataset==\"permuted\":\n",
        "      i_x_train,i_y_train,i_x_test,i_y_test=permute_MNIST(train_set[0],train_set[1],test_set[0],test_set[1],i)\n",
        "    elif dataset==\"split\":\n",
        "      i_x_train,i_y_train,i_x_test,i_y_test=split_MNIST(train_set[0],train_set[1],test_set[0],test_set[1],i)\n",
        "    X_trains.append(i_x_train)\n",
        "    Y_trains.append(i_y_train)\n",
        "    Xk_trains.append(i_x_train)\n",
        "    Yk_trains.append(i_y_train)\n",
        "    X_tests.append(i_x_test)\n",
        "    Y_tests.append(i_y_test)\n",
        "  X_coresets,Xk_coresets,Y_coresets,Yk_coresets=[],[],[],[]\n",
        "  if dataset==\"split\":\n",
        "    for i in range(nb_tasks):\n",
        "      Xk_trains[i],Yk_trains[i],Xk_coresets,Yk_coresets=k_center(Xk_trains[i],Yk_trains[i],X_coresets,Y_coresets,coreset_size)\n",
        "      X_trains[i],Y_trains[i],X_coresets,Y_coresets=random_coreset(X_trains[i],Y_trains[i],X_coresets,Y_coresets,coreset_size)\n",
        "  elif dataset==\"permuted\":\n",
        "    ids=k_center(Xk_trains[i],Yk_trains[i],X_coresets,Y_coresets,coreset_size,dataset=\"permuted\")\n",
        "    for i in range(nb_tasks):\n",
        "      Xk_trains[i],Yk_trains[i],Xk_coresets,Yk_coresets=np.delete(Xk_trains[i],ids,axis=0),np.delete(Yk_trains[i],ids,axis=0),Xk_coresets+[Xk_trains[i][ids,:]],Yk_coresets+[Yk_trains[i][ids]]\n",
        "      X_trains[i],Y_trains[i],X_coresets,Y_coresets=random_coreset(X_trains[i],Y_trains[i],X_coresets,Y_coresets,coreset_size)\n",
        "  #Make lists of dataloaders\n",
        "  train_loaders=dataloader(X_trains,Y_trains,batch_size)\n",
        "  train_loaders1=dataloader(X_trains,Y_trains,1)\n",
        "  train_loadersk=dataloader(Xk_trains,Yk_trains,batch_size)\n",
        "  train_loadersk1=dataloader(Xk_trains,Yk_trains,1)\n",
        "  test_loaders=dataloader(X_tests,Y_tests,batch_size)\n",
        "  if dataset==\"permuted\":\n",
        "    coreset_loaders=aggdataloader(X_coresets,Y_coresets,batch_size)\n",
        "    kcoreset_loaders=aggdataloader(Xk_coresets,Yk_coresets,batch_size)\n",
        "  elif dataset==\"split\":\n",
        "    coreset_loaders=dataloader(X_coresets,Y_coresets,batch_size)\n",
        "  return train_loaders,test_loaders,coreset_loaders,train_loaders1,train_loadersk,kcoreset_loaders,train_loadersk1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcUeMuriweu1"
      },
      "source": [
        "##Train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "b1a2BZ7TdN_S"
      },
      "outputs": [],
      "source": [
        "def train(model,loader,optimizer,num_epochs,device,clip_value,task_id,obj=\"nll\",is_print=True,lambda_ewc=50.,lambda_si=50000000.):\n",
        "  model.train()\n",
        "  n_data=get_total_elements(loader)\n",
        "  if obj==\"si\":\n",
        "    if task_id==0:\n",
        "      model.SI=SynapticIntelligence(model)\n",
        "      model.SI.init_SI()\n",
        "    model.SI.prepare_w_P_SI()\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0.\n",
        "    running_kldiv=0.\n",
        "    j=0\n",
        "    for inputs, targets in loader:\n",
        "      j+=1\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "      if obj==\"nll\":\n",
        "        loss=model.nll(inputs,targets)\n",
        "      elif obj==\"elbo\":\n",
        "        loss=model.elbo(inputs,targets,n_data)\n",
        "      elif obj==\"ewc\":\n",
        "        loss=model.nll(inputs,targets)+lambda_ewc*model.calculate_ewc()\n",
        "      elif obj==\"si\":\n",
        "        if task_id==0:\n",
        "          loss=model.nll(inputs,targets)\n",
        "        else:\n",
        "          loss=model.nll(inputs,targets)+lambda_si*model.SI.surrogate_l()\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "      optimizer.step()\n",
        "      if obj==\"si\":\n",
        "        model.SI.update_parameter_importance()\n",
        "      running_loss += loss.item()\n",
        "    if is_print:\n",
        "      print(f\"Epoch {epoch}: Loss: {running_loss/n_data}, KL-Div : {running_kldiv/n_data}\")\n",
        "\n",
        "def train_split(model,loader,optimizer,num_epochs,device,clip_value,task_id,obj=\"nll\",is_print=True,lambda_ewc=20,lambda_si=0.2):\n",
        "  model.train()\n",
        "  n_data=get_total_elements(loader)\n",
        "  if obj==\"si\":\n",
        "    if task_id==0:\n",
        "      model.SI=SynapticIntelligence(model)\n",
        "      model.SI.init_SI()\n",
        "    model.SI.prepare_w_P_SI()\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0.\n",
        "    running_kldiv=0.\n",
        "    j=0\n",
        "    for inputs, targets in loader:\n",
        "      j+=1\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "      if obj==\"nll\":\n",
        "        loss=model.nll(inputs,targets,task_id)\n",
        "      elif obj==\"elbo\":\n",
        "        loss=model.elbo(inputs,targets,n_data,task_id)\n",
        "      elif obj==\"ewc\":\n",
        "        loss=model.nll(inputs,targets,task_id)+lambda_ewc*model.calculate_ewc()\n",
        "      elif obj==\"si\":\n",
        "        if task_id==0:\n",
        "          loss=model.nll(inputs,targets,task_id)\n",
        "        else:\n",
        "          loss=model.nll(inputs,targets,task_id)+lambda_si*model.SI.surrogate_l()\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "      optimizer.step()\n",
        "      if obj==\"si\":\n",
        "        model.SI.update_parameter_importance()\n",
        "      running_loss += loss.item()\n",
        "    if is_print:\n",
        "      print(f\"Epoch {epoch}: Loss: {running_loss/n_data}, KL-Div : {running_kldiv/n_data}\")\n",
        "\n",
        "def test(model,loader,device,task_id=None,n_samples=100,dataset='split'):\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  n_data=get_total_elements(loader)\n",
        "  correct = 0\n",
        "  rmses=[]\n",
        "  with torch.no_grad():\n",
        "    for inputs, targets in loader:\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "      if model.output==\"categorical\":\n",
        "        if task_id is not None:\n",
        "          outputs = model(inputs,task_id,n_samples)\n",
        "        else:\n",
        "          if model.dist==\"gaussian\":\n",
        "            outputs=model(inputs,T=10)\n",
        "          elif model.dist==\"radial\" or model.dist==\"laplace\":\n",
        "            outputs=model(inputs,T=10)\n",
        "        predicted=outputs.mean(dim=0).argmax(dim=1)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "      if model.output==\"gaussian\":\n",
        "        rmse=model.rmse_gaussian(inputs,targets)\n",
        "        rmses.append(rmse.item())\n",
        "        mu,rho=model(inputs,n_samples)\n",
        "        outputs=mu.mean(dim=0)\n",
        "        predicted=outputs.argmax(dim=1)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "  if model.output==\"categorical\":\n",
        "    return correct/n_data\n",
        "  elif model.output==\"gaussian\":\n",
        "    return np.mean(rmses),correct/n_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pI_FcR5owhRA"
      },
      "source": [
        "##Core VCL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "l7adXZmIetXQ"
      },
      "outputs": [],
      "source": [
        "def run_vcl(\n",
        "    model, train_loaders, coreset_loaders, test_loaders,train_loaders1, epochs,epochs_coresets, device,lr=0.001,dataset=\"permuted\",obj=\"elbo\",clip_value = 10.,\n",
        "lambda_ewc=200.):\n",
        "\n",
        "    ACC = np.zeros((len(train_loaders),len(train_loaders)))\n",
        "    RMSE=np.zeros((len(train_loaders),len(train_loaders)))\n",
        "    optimizer=None\n",
        "    model.to(device)\n",
        "    if obj==\"elbo\":\n",
        "      objeval=\"elbo\"\n",
        "    else:\n",
        "      objeval=\"nll\"\n",
        "\n",
        "    for i in range(len(train_loaders)):\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "        if dataset==\"permuted\":\n",
        "          train(model, train_loaders[i], optimizer, epochs, device, task_id=i,obj=obj,clip_value=clip_value,lambda_ewc=lambda_ewc)\n",
        "        elif dataset==\"split\":\n",
        "          train_split(model, train_loaders[i], optimizer, epochs, device, task_id=i,obj=obj,clip_value=clip_value,lambda_ewc=lambda_ewc)\n",
        "        model.update_prior()\n",
        "\n",
        "        eval_model = copy.deepcopy(model)\n",
        "        optimizer = torch.optim.Adam(eval_model.parameters(), lr=lr)\n",
        "\n",
        "        if dataset==\"permuted\":\n",
        "          train(eval_model, coreset_loaders[i], optimizer, epochs_coresets, device,task_id=i, obj=objeval,is_print=False,clip_value=clip_value)\n",
        "        elif dataset==\"split\":\n",
        "          for k in range (0,i+1):\n",
        "            train_split(eval_model, coreset_loaders[k], optimizer, epochs_coresets, device,task_id=k, obj=objeval,is_print=False,clip_value=clip_value)\n",
        "\n",
        "        if dataset==\"permuted\":\n",
        "          if model.output==\"gaussian\":\n",
        "            for k in range(i+1):\n",
        "                RMSE[i][k],ACC[i][k]=test(eval_model, test_loaders[k], device)\n",
        "            print(\"RMSE\",RMSE[i])\n",
        "            print(\"ACC\",ACC[i])\n",
        "          elif model.output==\"categorical\":\n",
        "            for k in range(i+1):\n",
        "                ACC[i][k]=test(eval_model, test_loaders[k], device)\n",
        "\n",
        "            print(\"ACC\",ACC[i])\n",
        "        elif dataset==\"split\":\n",
        "          for k in range(i+1):\n",
        "              ACC[i][k]=test(eval_model, test_loaders[k], device,k)\n",
        "        wandb.log({\"average_accuracy\":ACC[i][0:i+1].mean()})\n",
        "        if obj==\"ewc\":\n",
        "          if dataset==\"permuted\":\n",
        "            model.calculate_fim(train_loaders1[i])\n",
        "          if dataset==\"split\":\n",
        "            model.calculate_fim(train_loaders1[i],task_id=i)\n",
        "        elif obj==\"si\":\n",
        "          model.SI.update_omega()\n",
        "          model.SI.divide_omega()\n",
        "\n",
        "    return ACC,RMSE,model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFNL10iewnXd"
      },
      "source": [
        "#Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "L9h9Oy849Z9x",
        "outputId": "710c0c2e-be03-4e32-e2d1-dc091c3b2a6c"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIb_0YtKhghU"
      },
      "outputs": [],
      "source": [
        "#Make sure to include the file mnist.pkl.gz in your working directory (available at https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz)\n",
        "with gzip.open('sample_data/mnist.pkl.gz', 'rb') as f:\n",
        "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "y_yF-yhg4CfE"
      },
      "outputs": [],
      "source": [
        "epochs=30\n",
        "init_rho=-4.\n",
        "prior_rho=-0.\n",
        "batch_size=512\n",
        "n_tasks=10\n",
        "lr=0.002\n",
        "epochs_coresets=0\n",
        "n_fisher_samples=3000\n",
        "obj=\"si\"\n",
        "k_center_bool=False\n",
        "output=\"gaussian\"\n",
        "bayesian=True\n",
        "dataset=\"permuted\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "agDq2XAsHp03"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "5wBXbbIgLuhe"
      },
      "outputs": [],
      "source": [
        "coreset_size=200\n",
        "config={                         # Track hyperparameters and metadata\n",
        "        \"epochs\":epochs,\n",
        "        \"init_rho\":init_rho,\n",
        "        \"prior_rho\":prior_rho,\n",
        "        \"batch_size\":batch_size,\n",
        "        \"n_tasks\":n_tasks,\n",
        "        \"lr\":lr,\n",
        "        \"coreset_size\":coreset_size,\n",
        "        \"epochs_coresets\":epochs_coresets,\n",
        "        \"n_fisher_samples\":n_fisher_samples,\n",
        "        \"obj\":obj,\n",
        "        \"k_center_bool\":k_center_bool\n",
        "    }\n",
        "\n",
        "train_loaders,test_loaders,coreset_loaders,train_loaders1,train_loadersk,kcoreset_loaders,train_loadersk1=loaders(n_tasks,coreset_size,batch_size=batch_size,dataset=dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ap9_GzLchk5j",
        "outputId": "e7dbc5d6-9353-4035-bb89-b932c4c79f7a"
      },
      "outputs": [],
      "source": [
        "torch.autograd.set_detect_anomaly(False)\n",
        "run = wandb.init(\n",
        "  project=\"new_rho_influence\",    # Specify your project\n",
        "  config=config,\n",
        ")\n",
        "for a in [-5.]:\n",
        "  print(\"init_rho\",a,\"prior_rho\",prior_rho)\n",
        "  print(\"__________________-\")\n",
        "  model=BayesianFC(784,20,100,2,device=device,init_rho=a,prior_rho=prior_rho,n_fisher_samples=n_fisher_samples,dist=\"gaussian\",output=output,bayesian=bayesian)\n",
        "\n",
        "  if k_center_bool:\n",
        "    ACC2=run_vcl(\n",
        "        model, train_loadersk, kcoreset_loaders, test_loaders,train_loadersk1, epochs,epochs_coresets, device,lr=lr,obj=obj\n",
        "    )\n",
        "  else:\n",
        "    ACC2,RMSE,model=run_vcl(\n",
        "        model, train_loaders, coreset_loaders, test_loaders,train_loaders1, epochs,epochs_coresets, device,lr=lr,obj=obj\n",
        "    )\n",
        "print(ACC2)\n",
        "print(RMSE)\n",
        "wandb.finish()\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "LRjqmnn3fvGV",
        "zZJtjBLgpmrR",
        "nEe-B0YAf-YP",
        "vfSYIEvROEly",
        "X46zSkUjBWN4",
        "luofCWfOXUDw",
        "e-U6YwBGXXWs",
        "wFNL10iewnXd"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
