{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ7duxALqG40"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egT5XTvDzqkr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import pickle\n",
        "import gzip\n",
        "import copy\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ln4SVXq5zuKS"
      },
      "outputs": [],
      "source": [
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzRB6AX7zyG7"
      },
      "source": [
        "#Misc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHbi0zCfzxUr"
      },
      "outputs": [],
      "source": [
        "def KL_DIV(mu0,rho0,mu1,rho1):\n",
        "  std1 = torch.exp(rho1)\n",
        "  std0 = torch.exp(rho0)\n",
        "  kl_div = (0.5*((std0/(1e-12+std1))**2+((mu0-mu1)**2/(1e-12+std1**2))-1+2*torch.log(std1/(1e-12+std0)))).sum()\n",
        "  return kl_div\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qu6cOUo6zz-q"
      },
      "outputs": [],
      "source": [
        "class BayesianLinear(nn.Module):\n",
        "    def __init__(self, input_size, output_size,device,init_rho,prior_rho,dist=\"gaussian\"):\n",
        "        super(BayesianLinear, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.device=device\n",
        "        self.dist=dist\n",
        "\n",
        "        self.post_w_mu = nn.Parameter(torch.empty(output_size, input_size))\n",
        "        self.post_b_mu = nn.Parameter(torch.empty(output_size))\n",
        "\n",
        "        self.post_w_rho = nn.Parameter(torch.full((output_size, input_size), init_rho))\n",
        "        self.post_b_rho = nn.Parameter(torch.full((output_size,), init_rho))\n",
        "\n",
        "        self.prior_w_mu = torch.zeros(output_size, input_size).to(device)\n",
        "        self.prior_b_mu = torch.zeros(output_size).to(device)\n",
        "\n",
        "        self.prior_w_rho = torch.full((output_size, input_size), prior_rho).to(device)\n",
        "        self.prior_b_rho = torch.full((output_size,), prior_rho).to(device)\n",
        "\n",
        "        self.init_parameters()\n",
        "\n",
        "\n",
        "    def to(self, device):\n",
        "        super(BayesianLinear, self).to(device)\n",
        "        self.prior_w_mu = self.prior_w_mu.to(device)\n",
        "        self.prior_b_mu = self.prior_b_mu.to(device)\n",
        "        self.prior_w_rho = self.prior_w_rho.to(device)\n",
        "        self.prior_b_rho = self.prior_b_rho.to(device)\n",
        "\n",
        "\n",
        "    def init_parameters(self):\n",
        "        nn.init.trunc_normal_(self.post_w_mu,std=0.1)\n",
        "        nn.init.trunc_normal_(self.post_b_mu,std=0.1)\n",
        "        nn.init.trunc_normal_(self.prior_w_mu,std=0.1)\n",
        "        nn.init.trunc_normal_(self.prior_b_mu,std=0.1)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "      a = self.sample_activation(x)\n",
        "      b = self.sample_b(x)\n",
        "      return a+b\n",
        "\n",
        "\n",
        "    def sample_activation(self,x):\n",
        "      if self.dist==\"gaussian\":\n",
        "        act_mean = F.linear(x,self.post_w_mu) #(batch_size,output_size)\n",
        "        std = torch.exp(self.post_w_rho) #(output_size,input_size)\n",
        "        act_std = torch.sqrt(1e-8+F.linear((x**2),(std**2))) #(batch_size,output_size)\n",
        "        act_eps = torch.randn(x.shape[0],1,self.output_size).to(self.device)\n",
        "        act = act_mean+act_std*act_eps\n",
        "        return act\n",
        "      if self.dist==\"radial\":\n",
        "        eps=torch.randn(self.post_w_rho.shape,device=self.device)\n",
        "        eps=eps/torch.linalg.norm(eps)\n",
        "        weights=self.post_w_mu+np.random.randn()*eps*torch.exp(0.5*self.post_w_rho)\n",
        "        act=F.linear(x,weights)\n",
        "        return act\n",
        "\n",
        "\n",
        "    def sample_weights_radial(self,n_samples):\n",
        "      eps=torch.randn((n_samples,*self.post_w_rho.shape),device=self.device)\n",
        "      eps=eps/torch.linalg.norm(eps,dim=(1,2)).view(-1,1,1)\n",
        "      weights=self.post_w_mu.view(1,*self.post_w_mu.shape)+np.random.randn()*eps*torch.exp(self.post_w_rho)\n",
        "      return weights\n",
        "\n",
        "\n",
        "    def Lcrossentropy(self,n_samples):\n",
        "      norms=torch.linalg.norm((self.sample_weights_radial(n_samples)-self.prior_w_mu.unsqueeze(0))/torch.exp(self.prior_w_rho.unsqueeze(0)),dim=(1,2))**2\n",
        "      L=-0.5*torch.mean(norms)\n",
        "      return L\n",
        "\n",
        "\n",
        "    def sample_b(self,x):\n",
        "      if self.dist==\"gaussian\":\n",
        "        b_eps = torch.randn(x.shape[0],1,self.output_size).to(self.device)\n",
        "        b = self.post_b_mu.view(1,1,-1)+torch.exp(self.post_b_rho.view(1,1,-1))*b_eps\n",
        "        return b\n",
        "\n",
        "\n",
        "    def update_prior(self):\n",
        "      self.prior_w_mu=copy.deepcopy(self.post_w_mu.data)\n",
        "      self.prior_w_rho=copy.deepcopy(self.post_w_rho.data)\n",
        "      self.prior_b_mu=copy.deepcopy(self.post_b_mu.data)\n",
        "      self.prior_b_rho=copy.deepcopy(self.post_b_rho.data)\n",
        "\n",
        "\n",
        "    def kl_div(self):\n",
        "      return(KL_DIV(self.post_w_mu,self.post_w_rho,self.prior_w_mu,self.prior_w_rho)+KL_DIV(self.post_b_mu,self.post_b_rho,self.prior_b_mu,self.prior_b_rho))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pY9gYozG0Di2"
      },
      "outputs": [],
      "source": [
        "class MultiGenerativeBayesianFC(nn.Module):\n",
        "  def __init__(self,input_size=784,hidden_size=500,n_hidden=2,z_size=50,init_rho=0,prior_rho=0,n_tasks=5,n_fisher_samples=3000):\n",
        "    super(MultiGenerativeBayesianFC,self).__init__()\n",
        "    self.device=device\n",
        "    self.encoders=nn.ModuleList([nn.ModuleList() for i in range (n_tasks)])\n",
        "    dims_encoder=[input_size]+(n_hidden*2-1)*[hidden_size]+[2*z_size]\n",
        "\n",
        "    for i in range (len(dims_encoder)-1):\n",
        "      for encoder in self.encoders:\n",
        "        encoder.append(nn.Linear(dims_encoder[i],dims_encoder[i+1],device=device))\n",
        "\n",
        "    self.generator_heads=nn.ModuleList([nn.ModuleList() for i in range (n_tasks)])\n",
        "    dims_genheads=[z_size]+n_hidden*[hidden_size]\n",
        "\n",
        "    for i in range (len(dims_genheads)-1):\n",
        "      for head in self.generator_heads:\n",
        "        head.append(BayesianLinear(input_size=dims_genheads[i],output_size=dims_genheads[i+1],init_rho=init_rho,prior_rho=prior_rho,device=device))\n",
        "\n",
        "    self.generator_shared=nn.ModuleList()\n",
        "    dims_genshared=n_hidden*[hidden_size]+[input_size]\n",
        "\n",
        "    for i in range (len(dims_genshared)-1):\n",
        "      self.generator_shared.append(BayesianLinear(input_size=dims_genshared[i],output_size=dims_genshared[i+1],init_rho=init_rho,prior_rho=prior_rho,device=device))\n",
        "\n",
        "    self.n_fishersamples=n_fisher_samples\n",
        "    self.fisher_dicts=[]\n",
        "    self.param_history=[]\n",
        "    self.SI=None\n",
        "\n",
        "\n",
        "  def sample_gaussian(self,mu,rho):\n",
        "    eps = torch.randn(mu.shape,device=device)\n",
        "    return mu+torch.exp(rho)*eps\n",
        "\n",
        "\n",
        "  def encode(self,x,id_task):\n",
        "    encoder=self.encoders[id_task]\n",
        "    for i,layer in enumerate(encoder):\n",
        "      x=layer(x)\n",
        "      if i<len(encoder)-1:\n",
        "        x=F.leaky_relu(x)\n",
        "    mu,logvar=x[:,:,:int(x.shape[2]/2)],x[:,:,int(x.shape[2]/2):]\n",
        "    return mu,logvar\n",
        "\n",
        "\n",
        "  def decode(self,mu,logvar,id_task):\n",
        "    genhead=self.generator_heads[id_task]\n",
        "    z_int=self.sample_gaussian(mu,logvar)\n",
        "    z=z_int\n",
        "    for i,layer in enumerate(genhead):\n",
        "      z=F.relu(layer(z))\n",
        "    for i,layer in enumerate(self.generator_shared):\n",
        "      z=layer(z)\n",
        "      if i<len(self.generator_shared)-1:\n",
        "        z=F.relu(z)\n",
        "      else:\n",
        "        z=F.sigmoid(z)\n",
        "    return z,z_int\n",
        "\n",
        "\n",
        "  def forward(self,x,id_task,n_samples=1):\n",
        "    if len(x.shape)==2:\n",
        "      x=x.unsqueeze(0).tile((n_samples,1,1))\n",
        "    mu,logvar=self.encode(x,id_task)\n",
        "    xout,z_int=self.decode(mu,logvar,id_task)\n",
        "    return xout\n",
        "\n",
        "\n",
        "  def kl_div(self,n_data,id_task):\n",
        "    total_kl = 0\n",
        "    for layer in self.generator_shared:\n",
        "        total_kl += layer.kl_div()\n",
        "    return total_kl/n_data\n",
        "\n",
        "\n",
        "  def logprob(self,x,id_task,n_samples,n_data,epsilon=1e-8):\n",
        "    batch_size=x.shape[0]\n",
        "    x=x.unsqueeze(0)\n",
        "    mu,logvar=self.encode(x,id_task)\n",
        "    mu=mu.tile((n_samples,1,1))\n",
        "    logvar=logvar.tile((n_samples,1,1))\n",
        "    xout,z_int=self.decode(mu,logvar,id_task)\n",
        "    KL=KL_DIV(mu,logvar,torch.zeros_like(mu).to(self.device),torch.zeros_like(logvar).to(self.device))\n",
        "    prob=torch.mul(torch.log(torch.clip(xout,min=1e-9,max=1)),x)\n",
        "    inv_prob=torch.mul(torch.log(torch.clip(1-xout,min=1e-9,max=1)),1-x)\n",
        "    inv_prob[inv_prob != inv_prob] = epsilon\n",
        "    return (torch.sum(torch.add(prob,inv_prob))-KL)/(batch_size*n_samples)\n",
        "\n",
        "\n",
        "  def elbo(self,x,id_task,n_data,n_samples=10,epsilon=1e-8):\n",
        "    nll=-self.logprob(x,id_task,n_samples,n_data,epsilon)\n",
        "    kl=self.kl_div(n_data,id_task)\n",
        "    return nll+kl,kl\n",
        "\n",
        "\n",
        "  def nll(self,x,id_task,n_data,n_samples=10,epsilon=1e-8):\n",
        "    return -self.logprob(x,id_task,n_samples,n_data,epsilon)\n",
        "\n",
        "\n",
        "  def calculate_fisher(self,train_loader1,task_id):\n",
        "    #Calculation of fisher matrix after each task\n",
        "    fisher_dic = {}\n",
        "    n_data=get_total_elements(train_loader1)\n",
        "    for n, p in self.named_parameters():\n",
        "      if n.startswith(\"generator_shared\"):\n",
        "        fisher_dic[n] = p.detach().clone().zero_()\n",
        "    self.eval()\n",
        "    for batch_id, (x, y) in enumerate(train_loader1):\n",
        "      if batch_id >= self.n_fishersamples:\n",
        "        break\n",
        "      x,y=x.to(self.device),y.to(self.device)\n",
        "      loss = self.nll(x,task_id,n_data)\n",
        "      self.zero_grad()\n",
        "      loss.backward()\n",
        "      for n, p in self.named_parameters():\n",
        "        if n.startswith(\"generator_shared\"):\n",
        "          if p.requires_grad and p.grad is not None:\n",
        "            fisher_dic[n] += p.grad.detach() ** 2\n",
        "\n",
        "    fisher_dic = {n: p/batch_id for n,p in fisher_dic.items()}\n",
        "    self.fisher_dicts.append(fisher_dic)\n",
        "    param_dic = {}\n",
        "    for n, p in self.named_parameters():\n",
        "      if n.startswith(\"generator_shared\"):\n",
        "        param_dic[n] = p.detach().clone()\n",
        "    self.param_history.append(param_dic)\n",
        "    self.train()\n",
        "\n",
        "\n",
        "  def calculate_ewc(self):\n",
        "    #calculation of ewc loss\n",
        "    Lloss = []\n",
        "    for task in range(len(self.fisher_dicts)):\n",
        "      for n, p in self.named_parameters():\n",
        "        if n.startswith(\"generator_shared\"):\n",
        "          if p.requires_grad:\n",
        "            Lloss.append( torch.sum( self.fisher_dicts[task][n] * ( p-self.param_history[task][n] ) ** 2 ))\n",
        "\n",
        "    return (1./2)*sum(Lloss)\n",
        "\n",
        "\n",
        "  def update_prior(self):\n",
        "    for head in self.generator_heads:\n",
        "      for layer in head:\n",
        "        layer.update_prior()\n",
        "    for layer in self.generator_shared:\n",
        "      layer.update_prior()\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "  def __init__(self,device,input_size=784,hidden_size=500,n_hidden=2,output_size=10):\n",
        "    super(Classifier,self).__init__()\n",
        "    self.layers=nn.ModuleList()\n",
        "    self.layers.append(nn.Linear(input_size,hidden_size,device=device))\n",
        "    for i in range(n_hidden-1):\n",
        "      self.layers.append(nn.Linear(hidden_size,hidden_size,device=device))\n",
        "    self.layers.append(nn.Linear(hidden_size,output_size,device=device))\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    for i,layer in enumerate(self.layers):\n",
        "      x=layer(x)\n",
        "      if i<len(self.layers)-1:\n",
        "        x=F.relu(x)\n",
        "    return F.log_softmax(x,dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjzeXdruI5lR"
      },
      "outputs": [],
      "source": [
        "class SynapticIntelligence():\n",
        "  def __init__(self,model,si_c=0.1,epsilon=0.1,gamma=0.9):\n",
        "    self.prev_SI={}\n",
        "    self.omega_SI={}\n",
        "    self.W={}\n",
        "    self.p_old={}\n",
        "    self.model=model\n",
        "    self.si_c = si_c\n",
        "    self.epsilon = epsilon\n",
        "    self.gamma=gamma\n",
        "\n",
        "  def init_SI(self):\n",
        "    #initialization step\n",
        "    for n, p in self.model.named_parameters():\n",
        "      if n.startswith(\"generator_shared\"):\n",
        "        if p.requires_grad:\n",
        "            self.prev_SI[n] = p.data.clone()\n",
        "\n",
        "  def prepare_w_P_SI(self):\n",
        "    #other initialization step\n",
        "    for n, p in self.model.named_parameters():\n",
        "      if n.startswith(\"generator_shared\"):\n",
        "        if p.requires_grad:\n",
        "            self.W[n] = p.data.clone().zero_()\n",
        "            self.p_old[n] = p.data.clone()\n",
        "\n",
        "  def update_parameter_importance(self):\n",
        "    #update the importance of parameters, in order to calculate omega\n",
        "    for n, p in self.model.named_parameters():\n",
        "      if n.startswith(\"generator_shared\"):\n",
        "        if p.requires_grad:\n",
        "            if p.grad is not None:\n",
        "                self.W[n].add_(-p.grad*(p.detach()-self.p_old[n]))\n",
        "            self.p_old[n] = p.detach().clone()\n",
        "\n",
        "  def update_omega(self):\n",
        "    #update of omega which allows to compute the regularization term\n",
        "        for n, p in self.model.named_parameters():\n",
        "          if n.startswith(\"generator_shared\"):\n",
        "            if p.requires_grad:\n",
        "                p_prev = self.prev_SI[n]\n",
        "                p_current = p.detach().clone()\n",
        "                p_change = p_current - p_prev\n",
        "                omega_add = self.W[n]/(p_change**2 + self.epsilon)\n",
        "                try:\n",
        "                    omega = self.omega_SI[n]\n",
        "                except KeyError:\n",
        "                    omega = p.detach().clone().zero_()\n",
        "                omega_new = omega + omega_add\n",
        "\n",
        "                self.prev_SI[n] = p_current\n",
        "                self.omega_SI[n] = omega_new\n",
        "\n",
        "  def divide_omega(self):\n",
        "    #potential division of omega to avoid divergence\n",
        "    pass\n",
        "    '''\n",
        "    for n, p in self.model.named_parameters():\n",
        "      if n.startswith(\"layers\"):\n",
        "        if p.requires_grad:\n",
        "          self.omega_SI[n]=self.omega_SI[n]*self.gamma'''\n",
        "\n",
        "  def surrogate_l(self):\n",
        "    #calculation of SI loss\n",
        "        try:\n",
        "            losses = []\n",
        "            for n, p in self.model.named_parameters():\n",
        "              if n.startswith(\"generator_shared\"):\n",
        "                if p.requires_grad:\n",
        "                    prev_values = self.prev_SI[n]\n",
        "                    omega = self.omega_SI[n]\n",
        "                    losses.append((omega * (p-prev_values)**2).sum())\n",
        "            return sum(losses)\n",
        "        except KeyError:\n",
        "            print(\"KeyError\")\n",
        "            return torch.tensor(0., device=self._device())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80yFpOCxJMov"
      },
      "source": [
        "#Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQ2nI0XuJNQb"
      },
      "outputs": [],
      "source": [
        "def permute_MNIST(x_train,y_train,x_test,y_test,task_id):\n",
        "    np.random.seed(task_id)\n",
        "    permutation = np.random.permutation(x_train.shape[1])\n",
        "    return x_train[:, permutation], y_train, x_test[:, permutation], y_test\n",
        "\n",
        "def split_MNIST(x_train,y_train,x_test,y_test,task_id):\n",
        "    Tasks=[[0,1],[2,3],[4,5],[6,7],[8,9]]\n",
        "    classes=Tasks[task_id]\n",
        "    idxs_train=np.where(np.logical_or(y_train==classes[0],y_train==classes[1]))[0]\n",
        "    idxs_test=np.where(np.logical_or(y_test==classes[0],y_test==classes[1]))[0]\n",
        "    return x_train[idxs_train],1*(y_train[idxs_train]==y_train[idxs_train][0]),x_test[idxs_test],1*(y_test[idxs_test]==y_train[idxs_train][0])\n",
        "\n",
        "def gen_mnist(x_train,y_train,x_test,y_test,task_id):\n",
        "    idxs_train=np.where(y_train==task_id)[0]\n",
        "    idxs_test=np.where(y_test==task_id)[0]\n",
        "\n",
        "    return x_train[idxs_train],y_train[idxs_train],x_test[idxs_test],y_test[idxs_test]\n",
        "\n",
        "\n",
        "''' Coreset methods '''\n",
        "\n",
        "def random_coreset(x_train, y_train, x_coreset, y_coreset, nb_coreset,dataset=\"split\"):\n",
        "\n",
        "    ids = np.random.choice(x_train.shape[0], nb_coreset, False)\n",
        "\n",
        "    x_coreset.append(x_train[ids,:])\n",
        "    y_coreset.append(y_train[ids])\n",
        "\n",
        "    x_train = np.delete(x_train, ids, axis=0)\n",
        "    y_train = np.delete(y_train, ids, axis=0)\n",
        "\n",
        "    return x_train, y_train, x_coreset, y_coreset\n",
        "\n",
        "def k_center(x_train, y_train, x_coreset, y_coreset, nb_coreset,dataset=\"split\"):\n",
        "\n",
        "    ids = []\n",
        "    visited = np.ones(x_train.shape[0])\n",
        "    id = 0\n",
        "\n",
        "    while len(ids)<nb_coreset:\n",
        "        ds = dists(x_train,id)*visited\n",
        "        id = np.atleast_1d(np.argmax(ds))[0]\n",
        "        visited[id]=0\n",
        "        ids.append(id)\n",
        "\n",
        "    x_coreset.append(x_train[ids,:])\n",
        "    y_coreset.append(y_train[ids])\n",
        "\n",
        "    x_train = np.delete(x_train, ids, axis=0)\n",
        "    y_train = np.delete(y_train, ids, axis=0)\n",
        "\n",
        "    if dataset==\"permuted\":\n",
        "      return ids\n",
        "    else:\n",
        "      return x_train, y_train, x_coreset, y_coreset\n",
        "\n",
        "def dists(x_train, id):\n",
        "    return np.linalg.norm(x_train-x_train[id,:],axis=1)\n",
        "\n",
        "def get_total_elements(loader):\n",
        "  total_elements = 0\n",
        "  for batch_idx, (data, target) in enumerate(loader):\n",
        "      total_elements += data.shape[0]  # Add the batch size to the total\n",
        "  return total_elements\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojS7xelqK6ly"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "def dataloader(X, Y, batch_size=128, shuffle=True, num_workers=0, pin_memory=False):\n",
        "    loaders = []\n",
        "    for x, y in zip(X, Y):\n",
        "        assert isinstance(x, np.ndarray) and isinstance(y, np.ndarray), \"X and Y must be NumPy arrays\"\n",
        "        dataset = TensorDataset(torch.from_numpy(x).float(), torch.from_numpy(y).long())\n",
        "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,\n",
        "                            num_workers=num_workers, pin_memory=pin_memory)\n",
        "        loaders.append(loader)\n",
        "    return loaders\n",
        "\n",
        "\n",
        "def aggdataloader(X, Y, batch_size=128, shuffle=True, num_workers=0, pin_memory=False):\n",
        "    loaders = []\n",
        "    x_agg,y_agg=None,None\n",
        "    for i,(x, y) in enumerate(zip(X, Y)):\n",
        "        if i==0:\n",
        "          x_agg=x\n",
        "          y_agg=y\n",
        "        else:\n",
        "          x_agg=np.concatenate((x_agg,x),axis=0)\n",
        "          y_agg=np.concatenate((y_agg,y),axis=0)\n",
        "        assert isinstance(x, np.ndarray) and isinstance(y, np.ndarray), \"X and Y must be NumPy arrays\"\n",
        "        dataset = TensorDataset(torch.from_numpy(x_agg).float(), torch.from_numpy(y_agg).long())\n",
        "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,\n",
        "                            num_workers=num_workers, pin_memory=pin_memory)\n",
        "        loaders.append(loader)\n",
        "    return loaders\n",
        "\n",
        "\n",
        "def loaders(nb_tasks,coreset_size,batch_size=128):\n",
        "  with gzip.open('sample_data/mnist.pkl.gz', 'rb') as f:\n",
        "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
        "  X_trains,Y_trains,Xk_trains,Yk_trains,X_tests,Y_tests=[],[],[],[],[],[]\n",
        "  for i in range(nb_tasks):\n",
        "    i_x_train,i_y_train,i_x_test,i_y_test=gen_mnist(train_set[0],train_set[1],test_set[0],test_set[1],i)\n",
        "    X_trains.append(i_x_train)\n",
        "    Y_trains.append(i_y_train)\n",
        "    Xk_trains.append(i_x_train)\n",
        "    Yk_trains.append(i_y_train)\n",
        "    X_tests.append(i_x_test)\n",
        "    Y_tests.append(i_y_test)\n",
        "  X_coresets,Xk_coresets,Y_coresets,Yk_coresets=[],[],[],[]\n",
        "  for i in range(nb_tasks):\n",
        "    Xk_trains[i],Yk_trains[i],Xk_coresets,Yk_coresets=k_center(Xk_trains[i],Yk_trains[i],X_coresets,Y_coresets,coreset_size)\n",
        "    X_trains[i],Y_trains[i],X_coresets,Y_coresets=random_coreset(X_trains[i],Y_trains[i],X_coresets,Y_coresets,coreset_size)\n",
        "  train_loaders=dataloader(X_trains,Y_trains,batch_size)\n",
        "  train_loaders1=dataloader(X_trains,Y_trains,1)\n",
        "  train_loadersk=dataloader(Xk_trains,Yk_trains,batch_size)\n",
        "  train_loadersk1=dataloader(Xk_trains,Yk_trains,1)\n",
        "  test_loaders=dataloader(X_tests,Y_tests,batch_size)\n",
        "  coreset_loaders=dataloader(X_coresets,Y_coresets,batch_size)\n",
        "  kcoreset_loaders=dataloader(Xk_coresets,Yk_coresets,batch_size)\n",
        "  return train_loaders,test_loaders,coreset_loaders,train_loaders1,train_loadersk,kcoreset_loaders,train_loadersk1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDIpj40VNfpa"
      },
      "outputs": [],
      "source": [
        "def train(model,loader,optimizer,num_epochs,device):\n",
        "  model.train()\n",
        "  n_data=get_total_elements(loader)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0.\n",
        "    running_kldiv=0.\n",
        "    j=0\n",
        "    for inputs, targets in loader:\n",
        "      inputs,targets=inputs.to(device),targets.to(device)\n",
        "      outputs=model(inputs)\n",
        "      loss=F.nll_loss(outputs,targets,reduce=\"mean\")\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item()\n",
        "    if epoch%10==0:\n",
        "      print(f\"Epoch {epoch}: Loss: {running_loss/n_data}, KL-Div : {running_kldiv/n_data}\")\n",
        "\n",
        "def train_split(model,loader,optimizer,num_epochs,device,clip_value,task_id,obj=\"nll\",is_print=True,lambda_ewc=20,lambda_si=100):\n",
        "  model.train()\n",
        "  n_data=get_total_elements(loader)\n",
        "  if obj==\"si\":\n",
        "    if task_id==0:\n",
        "      model.SI=SynapticIntelligence(model)\n",
        "      model.SI.init_SI()\n",
        "    model.SI.prepare_w_P_SI()\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0.\n",
        "    running_kldiv=0.\n",
        "    j=0\n",
        "    for inputs, targets in loader:\n",
        "      j+=1\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "      if obj==\"nll\":\n",
        "        loss=model.nll(inputs,task_id,n_data)\n",
        "      elif obj==\"elbo\":\n",
        "        loss,kl_div=model.elbo(inputs,task_id,n_data)\n",
        "        running_kldiv+=kl_div\n",
        "      elif obj==\"ewc\":\n",
        "        loss=model.nll(inputs,task_id,n_data)+lambda_ewc*model.calculate_ewc()\n",
        "      elif obj==\"si\":\n",
        "        if task_id==0:\n",
        "          loss=model.nll(inputs,task_id,n_data)\n",
        "        else:\n",
        "          loss=model.nll(inputs,task_id,n_data)+lambda_si*model.SI.surrogate_l()\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "      optimizer.step()\n",
        "      if obj==\"si\":\n",
        "        model.SI.update_parameter_importance()\n",
        "      running_loss += loss.item()\n",
        "    if is_print and epoch%10==0:\n",
        "      print(f\"Epoch {epoch}: Loss: {running_loss/n_data}, KL-Div : {running_kldiv/n_data}\")\n",
        "\n",
        "def test(model,loader,device,classifier,n_plots=10,task_id=None,n_samples=100,dataset='split'):\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  n_data=get_total_elements(loader)\n",
        "  print=True\n",
        "  nll=[]\n",
        "  uncertainty=[]\n",
        "  img=None\n",
        "  with torch.no_grad():\n",
        "    for inputs, targets in loader:\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "      xout = model(inputs,task_id,n_samples=1)\n",
        "      probs=classifier(xout.squeeze(0))\n",
        "      uncertainty.append(F.nll_loss(probs,targets,reduce=\"mean\").item())\n",
        "      if print:\n",
        "        plot_digits(xout.squeeze(0)[0:n_plots])\n",
        "        print=False\n",
        "      img=xout.squeeze(0)[0].cpu().detach().numpy()\n",
        "      nllval=model.nll(inputs,task_id,n_data)\n",
        "      nll.append(nllval.item())\n",
        "  return(-np.mean(nll),np.mean(uncertainty),img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMB-259sNTxG"
      },
      "outputs": [],
      "source": [
        "def run_vcl(\n",
        "    model, train_loaders, coreset_loaders, test_loaders,train_loaders1, epochs,epochs_coresets, device,classifier,lr=0.001,dataset=\"permuted\",obj=\"elbo\",clip_value = 10.,\n",
        "lambda_ewc=20.):\n",
        "\n",
        "    LL = np.zeros((len(train_loaders),len(train_loaders)))\n",
        "    U=np.zeros((len(train_loaders),len(train_loaders)))\n",
        "    IMGS=np.zeros((len(train_loaders),len(train_loaders),784))\n",
        "    optimizer=None\n",
        "    model.to(device)\n",
        "\n",
        "    for i in range(len(train_loaders)):\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "        train_split(model, train_loaders[i], optimizer, epochs, device, task_id=i,obj=obj,clip_value=clip_value,lambda_ewc=lambda_ewc)\n",
        "        model.update_prior()\n",
        "        for k in range(i+1):\n",
        "            LL[i][k],U[i][k],IMGS[i][k]=test(model, test_loaders[k], device,classifier,task_id=k)\n",
        "        if obj==\"ewc\":\n",
        "          model.calculate_fisher(train_loaders1[i],task_id=i)\n",
        "        elif obj==\"si\":\n",
        "          model.SI.update_omega()\n",
        "\n",
        "    return LL,U,IMGS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6myb8fVO5ob"
      },
      "outputs": [],
      "source": [
        "def plot_digits(x):\n",
        "  n_plots=x.shape[0]\n",
        "  images=x.view(-1,28,28)\n",
        "  fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
        "  for i, ax in enumerate(axes):\n",
        "      ax.imshow(images[i].detach().cpu().numpy(), cmap=\"gray\")\n",
        "      ax.axis(\"off\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZY3SZA9sQDF"
      },
      "source": [
        "#Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpHobgoVIZNt"
      },
      "outputs": [],
      "source": [
        "#Make sure to include the file mnist.pkl.gz in your working directory (available at https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz)\n",
        "with gzip.open('sample_data/mnist.pkl.gz', 'rb') as f:\n",
        "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qh2T1pQWLbd8"
      },
      "outputs": [],
      "source": [
        "epochs=200\n",
        "init_rho=-4.8\n",
        "prior_rho=0.\n",
        "batch_size=1024\n",
        "n_tasks=10\n",
        "lr=0.01\n",
        "coreset_size=200\n",
        "epochs_coresets=0\n",
        "n_fisher_samples=3000\n",
        "obj=\"ewc\"\n",
        "k_center_bool=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzO6V9TMIg-_"
      },
      "outputs": [],
      "source": [
        "train_loaders,test_loaders,coreset_loaders,train_loaders1,train_loadersk,kcoreset_loaders,train_loadersk1=loaders(n_tasks,coreset_size,batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uKDjGxg6s_P"
      },
      "outputs": [],
      "source": [
        "classifier=Classifier(device)\n",
        "loader=dataloader(np.array([train_set[0]]),np.array([train_set[1]]),batch_size=512)[0]\n",
        "optimizer=torch.optim.Adam(classifier.parameters(),lr=lr)\n",
        "train(classifier,loader,optimizer,epochs,device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94i9r4IzjkhZ"
      },
      "outputs": [],
      "source": [
        "test_loader=dataloader(np.array([test_set[0]]),np.array([test_set[1]]),batch_size=512)[0]\n",
        "correct=0\n",
        "n_data=get_total_elements(test_loader)\n",
        "with torch.no_grad():\n",
        "  for inputs, targets in test_loader:\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    xout = classifier(inputs)\n",
        "    pred = xout.argmax(dim=1)\n",
        "    correct += (1*(pred==targets)).sum().item()\n",
        "print(correct/n_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KHgaOtwMRNF"
      },
      "outputs": [],
      "source": [
        "init_rho=-5.\n",
        "model=MultiGenerativeBayesianFC(init_rho=init_rho,prior_rho=prior_rho,n_tasks=10)\n",
        "obj='elbo'\n",
        "LLvcl,Uvcl,IMGSvcl=run_vcl(\n",
        "      model, train_loaders, coreset_loaders, test_loaders,train_loaders1, epochs,epochs_coresets, device,classifier,lr=lr,obj=obj\n",
        "  )\n",
        "model=MultiGenerativeBayesianFC(init_rho=init_rho,prior_rho=prior_rho,n_tasks=10)\n",
        "obj=\"ewc\"\n",
        "LLewc,Uewc,IMGSewc=run_vcl(\n",
        "      model, train_loaders, coreset_loaders, test_loaders,train_loaders1, epochs,epochs_coresets, device,classifier,lr=lr,obj=obj\n",
        "  )\n",
        "model=MultiGenerativeBayesianFC(init_rho=init_rho,prior_rho=prior_rho,n_tasks=10)\n",
        "obj=\"si\"\n",
        "LLsi,Usi,IMGSsi=run_vcl(\n",
        "      model, train_loaders, coreset_loaders, test_loaders,train_loaders1, epochs,epochs_coresets, device,classifier,lr=lr,obj=obj\n",
        "  )\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ZZ7duxALqG40",
        "XzRB6AX7zyG7",
        "80yFpOCxJMov",
        "WZY3SZA9sQDF"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
